{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssF0pr85vgUy"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, MultiHeadAttention, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "G0bczQlEvy-p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(Model):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, d_model)\n",
        "        self.transformer = [\n",
        "            MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.fc = Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        src, tgt = inputs\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        for layer in self.transformer:\n",
        "            tgt = layer(tgt, src, src, training=training)\n",
        "        x = self.fc(tgt)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "P3Wlh5YCxWze"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\"this is a simple example\", \"this is another example\"]\n",
        "target_texts = [\"simple example\", \"another example\"]\n",
        "\n",
        "tokenizer = Tokenizer(filters=\"\", lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "input_seqs = tokenizer.texts_to_sequences(input_texts)\n",
        "target_seqs = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "input_seqs = pad_sequences(input_seqs, padding=\"post\")\n",
        "target_seqs = pad_sequences(target_seqs, padding=\"post\")\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "tKaLZ697v-wA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 64\n",
        "nhead = 4\n",
        "num_layers = 2\n",
        "\n",
        "model = SimpleTransformer(vocab_size, d_model, nhead, num_layers)"
      ],
      "metadata": {
        "id": "YCIJ4GAiwCoe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 2\n",
        "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=loss_fn)\n",
        "model.fit(x=[input_seqs, target_seqs[:, :-1]], y=target_seqs[:, 1:], batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWu1tQYywGC8",
        "outputId": "a5bf9c20-a69f-4d35-d35f-e111584815de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9455\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9070\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8663\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8203\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7666\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7040\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6313\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5478\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4529\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3463\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2283\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0996\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9621\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8187\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6740\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5337\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4043\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2920\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2010\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1323\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0839\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0518\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0315\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0190\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0115\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0070\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0043\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0027\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0018\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0012\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.7289e-04\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.3022e-04\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.7216e-04\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.6723e-04\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9608e-04\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4709e-04\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1247e-04\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.7734e-05\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.9675e-05\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.6325e-05\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.6252e-05\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 3.8742e-05\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.2782e-05\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8193e-05\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.4378e-05\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1577e-05\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9014e-05\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7166e-05\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5497e-05\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4186e-05\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3053e-05\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2040e-05\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1265e-05\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0550e-05\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0014e-05\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.5963e-06\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.1195e-06\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.7022e-06\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.3446e-06\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.1062e-06\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.7486e-06\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.4506e-06\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.3909e-06\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.1525e-06\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 7.0929e-06\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.9737e-06\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.8545e-06\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.6161e-06\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.5565e-06\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.4969e-06\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.3181e-06\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.2585e-06\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.1989e-06\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.1393e-06\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.1393e-06\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 6.0201e-06\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.0201e-06\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.0201e-06\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.0201e-06\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.9604e-06\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.9604e-06\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.9604e-06\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.9604e-06\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.9604e-06\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.9008e-06\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.9008e-06\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.8412e-06\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.8412e-06\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.8412e-06\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.8412e-06\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.8412e-06\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.7220e-06\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.7220e-06\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.7220e-06\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 5.7220e-06\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.7220e-06\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.6624e-06\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.6624e-06\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.6624e-06\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.6624e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b01612430>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(input_text, model, tokenizer, max_summary_length=10):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, padding=\"post\")\n",
        "    summary_seq = np.zeros((1, max_summary_length), dtype=np.int32)\n",
        "\n",
        "    for i in range(max_summary_length):\n",
        "        logits = model.predict([input_seq, summary_seq])\n",
        "        predicted_token = np.argmax(logits[0, i])\n",
        "        summary_seq[0, i] = predicted_token\n",
        "        if predicted_token == 0:\n",
        "            break\n",
        "\n",
        "    summary_text = tokenizer.sequences_to_texts(summary_seq)\n",
        "    return summary_text[0]"
      ],
      "metadata": {
        "id": "YpL4MdwLyPSk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"this is a simple example\"\n",
        "summary = summarize(input_text, model, tokenizer)\n",
        "print(f\"Input text: {input_text}\")\n",
        "print(f\"Generated summary: {summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejxi4ff5yRm-",
        "outputId": "afcde8ed-f8dc-4456-87a3-907b9cc0b891"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Input text: this is a simple example\n",
            "Generated summary: example example example example example example example example example example\n"
          ]
        }
      ]
    }
  ]
}